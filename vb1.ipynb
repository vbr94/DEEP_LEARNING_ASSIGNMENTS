{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B1: Federated Averaging.\n",
    "\n",
    "### Name: Vinayak Rai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read the lecture note: [click here](https://github.com/wangshusen/DeepLearning/blob/master/LectureNotes/Logistic/paper/logistic.pdf)\n",
    "\n",
    "2. Read, complete, and run my code.\n",
    "\n",
    "3. **Implement mini-batch SGD** and evaluate the performance.\n",
    "\n",
    "4. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain **the code** and **the output after execution**.\n",
    "    \n",
    "    * Missing **the output after execution** will not be graded.\n",
    "    \n",
    "5. Upload this .HTML file to your Google Drive, Dropbox, or your Github repo.  (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "6. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM2/HM2.html\n",
    "\n",
    "\n",
    "## Grading criteria:\n",
    "\n",
    "1. When computing the ```gradient``` and ```objective function value``` using a batch of samples, use **matrix-vector multiplication** rather than a FOR LOOP of **vector-vector multiplications**.\n",
    "\n",
    "2. Plot ```objective function value``` against ```epochs```. In the plot, compare GD, SGD, and MB-SGD (with $b=8$ and $b=64$). The plot must look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Diabete dataset from https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes\n",
    "- Load the data using sklearn.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (768, 8)\n",
      "Shape of y: (768,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy\n",
    "\n",
    "x_sparse, y = datasets.load_svmlight_file('diabetes')\n",
    "x = x_sparse.todense()\n",
    "\n",
    "print('Shape of x: ' + str(x.shape))\n",
    "print('Shape of y: ' + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Partition to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (640, 8)\n",
      "Shape of x_test: (128, 8)\n",
      "Shape of y_train: (640, 1)\n",
      "Shape of y_test: (128, 1)\n"
     ]
    }
   ],
   "source": [
    "# partition the data to training and test sets\n",
    "n = x.shape[0]\n",
    "n_train = 640\n",
    "n_test = n - n_train\n",
    "\n",
    "rand_indices = numpy.random.permutation(n)\n",
    "train_indices = rand_indices[0:n_train]\n",
    "test_indices = rand_indices[n_train:n]\n",
    "\n",
    "x_train = x[train_indices, :]\n",
    "x_test = x[test_indices, :]\n",
    "y_train = y[train_indices].reshape(n_train, 1)\n",
    "y_test = y[test_indices].reshape(n_test, 1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to trainsform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "[[-0.07187787  0.06006189 -0.09102497 -0.02397864  0.06039257 -0.0245659\n",
      "   0.15831709  0.07897736]]\n",
      "test std = \n",
      "[[0.91471416 0.97494103 1.13058742 1.01560775 1.10314909 1.05593675\n",
      "  1.34113806 1.04781731]]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "import numpy\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(numpy.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(numpy.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Add a dimension of all ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (640, 9)\n",
      "Shape of x_test: (128, 9)\n"
     ]
    }
   ],
   "source": [
    "n_train, d = x_train.shape\n",
    "x_train = numpy.concatenate((x_train, numpy.ones((n_train, 1))), axis=1)\n",
    "\n",
    "n_test, d = x_test.shape\n",
    "x_test = numpy.concatenate((x_test, numpy.ones((n_test, 1))), axis=1)\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective function value\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     objective function value (scalar)\n",
    "def objective(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "    yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "    vec1 = numpy.exp(-yxw) # n-by-1 matrix\n",
    "    vec2 = numpy.log(1 + vec1) # n-by-1 matrix\n",
    "    loss = numpy.mean(vec2) # scalar\n",
    "    reg = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    return loss + reg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial objective function value = 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# initialize w\n",
    "d = x_train.shape[1]\n",
    "w = numpy.zeros((d, 1))\n",
    "\n",
    "# evaluate the objective function value at w\n",
    "lam = 1E-6\n",
    "objval0 = objective(w, x_train, y_train, lam)\n",
    "print('Initial objective function value = ' + str(objval0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ is $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     g: g: d-by-1 matrix, full gradient\n",
    "def gradient(w, x, y, lam):\n",
    "    n, d = x.shape\n",
    "    yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "    yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "    vec1 = numpy.exp(yxw) # n-by-1 matrix\n",
    "    vec2 = numpy.divide(yx, 1+vec1) # n-by-d matrix\n",
    "    vec3 = -numpy.mean(vec2, axis=0).reshape(d, 1) # d-by-1 matrix\n",
    "    g = vec3 + lam * w\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     stepsize: scalar\n",
    "#     max_iter: integer, the maximal iterations\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each iteration's objective value\n",
    "def grad_descent(x, y, lam, stepsize, max_iter=100, w=None):\n",
    "    n, d = x.shape\n",
    "    objvals = numpy.zeros(max_iter) # store the objective values\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1)) # zero initialization\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        objval = objective(w, x, y, lam)\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at t=' + str(t) + ' is ' + str(objval))\n",
    "        g = gradient(w, x, y, lam)\n",
    "        w -= stepsize * g\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at t=0 is 0.6931471805599453\n",
      "Objective value at t=1 is 0.5964055161954884\n",
      "Objective value at t=2 is 0.5559403830038817\n",
      "Objective value at t=3 is 0.5340560479834882\n",
      "Objective value at t=4 is 0.5204153114045377\n",
      "Objective value at t=5 is 0.5112094807629206\n",
      "Objective value at t=6 is 0.5046826158970392\n",
      "Objective value at t=7 is 0.4998974924128015\n",
      "Objective value at t=8 is 0.49630241892214155\n",
      "Objective value at t=9 is 0.4935499471095543\n",
      "Objective value at t=10 is 0.49141039794175606\n",
      "Objective value at t=11 is 0.48972633792808834\n",
      "Objective value at t=12 is 0.48838675578324564\n",
      "Objective value at t=13 is 0.4873115572494209\n",
      "Objective value at t=14 is 0.4864418368933643\n",
      "Objective value at t=15 is 0.4857335583114122\n",
      "Objective value at t=16 is 0.4851533305728653\n",
      "Objective value at t=17 is 0.48467551783953594\n",
      "Objective value at t=18 is 0.4842802206660648\n",
      "Objective value at t=19 is 0.4839518407483266\n",
      "Objective value at t=20 is 0.4836780441964264\n",
      "Objective value at t=21 is 0.4834490019359235\n",
      "Objective value at t=22 is 0.483256825934966\n",
      "Objective value at t=23 is 0.4830951458322849\n",
      "Objective value at t=24 is 0.4829587875731833\n",
      "Objective value at t=25 is 0.482843527068685\n",
      "Objective value at t=26 is 0.482745899655647\n",
      "Objective value at t=27 is 0.48266305149463806\n",
      "Objective value at t=28 is 0.4825926227916482\n",
      "Objective value at t=29 is 0.48253265538547807\n",
      "Objective value at t=30 is 0.4824815191457269\n",
      "Objective value at t=31 is 0.48243785300488284\n",
      "Objective value at t=32 is 0.4824005174568701\n",
      "Objective value at t=33 is 0.4823685560997915\n",
      "Objective value at t=34 is 0.4823411643563351\n",
      "Objective value at t=35 is 0.4823176639231641\n",
      "Objective value at t=36 is 0.4822974818173411\n",
      "Objective value at t=37 is 0.48228013312972673\n",
      "Objective value at t=38 is 0.4822652067813636\n",
      "Objective value at t=39 is 0.4822523537229395\n",
      "Objective value at t=40 is 0.48224127712972636\n",
      "Objective value at t=41 is 0.482231724232421\n",
      "Objective value at t=42 is 0.4822234794937351\n",
      "Objective value at t=43 is 0.48221635889560305\n",
      "Objective value at t=44 is 0.48221020514570184\n",
      "Objective value at t=45 is 0.4822048836470669\n",
      "Objective value at t=46 is 0.4822002791027876\n",
      "Objective value at t=47 is 0.48219629265053887\n",
      "Objective value at t=48 is 0.4821928394401583\n",
      "Objective value at t=49 is 0.48218984658249664\n",
      "Objective value at t=50 is 0.4821872514100217\n",
      "Objective value at t=51 is 0.4821849999996968\n",
      "Objective value at t=52 is 0.48218304591690203\n",
      "Objective value at t=53 is 0.48218134914596567\n",
      "Objective value at t=54 is 0.48217987517848115\n",
      "Objective value at t=55 is 0.4821785942352474\n",
      "Objective value at t=56 is 0.48217748060152194\n",
      "Objective value at t=57 is 0.48217651205850004\n",
      "Objective value at t=58 is 0.48217566939661205\n",
      "Objective value at t=59 is 0.48217493599847067\n",
      "Objective value at t=60 is 0.48217429748118334\n",
      "Objective value at t=61 is 0.48217374138931324\n",
      "Objective value at t=62 is 0.48217325693110313\n",
      "Objective value at t=63 is 0.4821728347516869\n",
      "Objective value at t=64 is 0.48217246673795583\n",
      "Objective value at t=65 is 0.4821721458505381\n",
      "Objective value at t=66 is 0.48217186597902684\n",
      "Objective value at t=67 is 0.482171621817153\n",
      "Objective value at t=68 is 0.4821714087550886\n",
      "Objective value at t=69 is 0.482171222786471\n",
      "Objective value at t=70 is 0.4821710604280883\n",
      "Objective value at t=71 is 0.4821709186504619\n",
      "Objective value at t=72 is 0.48217079481781433\n",
      "Objective value at t=73 is 0.4821706866361261\n",
      "Objective value at t=74 is 0.48217059210816743\n",
      "Objective value at t=75 is 0.4821705094945507\n",
      "Objective value at t=76 is 0.4821704372799796\n",
      "Objective value at t=77 is 0.4821703741439884\n",
      "Objective value at t=78 is 0.4821703189355631\n",
      "Objective value at t=79 is 0.48217027065112034\n",
      "Objective value at t=80 is 0.4821702284153893\n",
      "Objective value at t=81 is 0.48217019146481155\n",
      "Objective value at t=82 is 0.4821701591331162\n",
      "Objective value at t=83 is 0.48217013083878574\n",
      "Objective value at t=84 is 0.48217010607415667\n",
      "Objective value at t=85 is 0.4821700843959409\n",
      "Objective value at t=86 is 0.48217006541697915\n",
      "Objective value at t=87 is 0.482170048799063\n",
      "Objective value at t=88 is 0.48217003424668614\n",
      "Objective value at t=89 is 0.48217002150160293\n",
      "Objective value at t=90 is 0.4821700103380869\n",
      "Objective value at t=91 is 0.48217000055880116\n",
      "Objective value at t=92 is 0.48216999199119637\n",
      "Objective value at t=93 is 0.48216998448437276\n",
      "Objective value at t=94 is 0.4821699779063411\n",
      "Objective value at t=95 is 0.48216997214163576\n",
      "Objective value at t=96 is 0.4821699670892294\n",
      "Objective value at t=97 is 0.4821699626607158\n",
      "Objective value at t=98 is 0.48216995877872143\n",
      "Objective value at t=99 is 0.4821699553755196\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6\n",
    "stepsize = 1.0\n",
    "w, objvals_gd1 = grad_descent(x_train, y_train, lam, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     xi: 1-by-d matrix\n",
    "#     yi: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    d = xi.shape[0]\n",
    "    yx = yi * xi # 1-by-d matrix\n",
    "    yxw = float(numpy.dot(yx, w)) # scalar\n",
    "    \n",
    "    # calculate objective function Q_i\n",
    "    loss = numpy.log(1 + numpy.exp(-yxw)) # scalar\n",
    "    reg = lam / 2 * numpy.sum(w * w) # scalar\n",
    "    obj = loss + reg\n",
    "    \n",
    "    # calculate stochastic gradient\n",
    "    g_loss = -yx.T / (1 + numpy.exp(yxw)) # d-by-1 matrix\n",
    "    g = g_loss + lam * w # d-by-1 matrix\n",
    "    \n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# Inputs:\n",
    "#     x: n-by-d matrix\n",
    "#     y: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     stepsize: scalar\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "#     w: d-by-1 matrix, initialization of w\n",
    "# Return:\n",
    "#     w: the solution\n",
    "#     objvals: record of each iteration's objective value\n",
    "def sgd(x, y, lam, stepsize, max_epoch=100, w=None):\n",
    "    n, d = x.shape\n",
    "    objvals = numpy.zeros(max_epoch) # store the objective values\n",
    "    if w is None:\n",
    "        w = numpy.zeros((d, 1)) # zero initialization\n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "        # randomly shuffle the samples\n",
    "        rand_indices = numpy.random.permutation(n)\n",
    "        x_rand = x[rand_indices, :]\n",
    "        y_rand = y[rand_indices, :]\n",
    "        \n",
    "        objval = 0 # accumulate the objective values\n",
    "        for i in range(n):\n",
    "            xi = x_rand[i, :] # 1-by-d matrix\n",
    "            yi = float(y_rand[i, :]) # scalar\n",
    "            obj, g = stochastic_objective_gradient(w, xi, yi, lam)\n",
    "            objval += obj\n",
    "            w -= stepsize * g\n",
    "        \n",
    "        stepsize *= 0.9 # decrease step size\n",
    "        objval /= n\n",
    "        objvals[t] = objval\n",
    "        print('Objective value at epoch t=' + str(t) + ' is ' + str(objval))\n",
    "    \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value at epoch t=0 is 0.552207876641306\n",
      "Objective value at epoch t=1 is 0.5270982246099403\n",
      "Objective value at epoch t=2 is 0.5208723623331262\n",
      "Objective value at epoch t=3 is 0.5212041878076763\n",
      "Objective value at epoch t=4 is 0.513292838458776\n",
      "Objective value at epoch t=5 is 0.5101621488857637\n",
      "Objective value at epoch t=6 is 0.5126937147111005\n",
      "Objective value at epoch t=7 is 0.5040896450535614\n",
      "Objective value at epoch t=8 is 0.5068122828751764\n",
      "Objective value at epoch t=9 is 0.5088581963907949\n",
      "Objective value at epoch t=10 is 0.5056937792528515\n",
      "Objective value at epoch t=11 is 0.5030604498048017\n",
      "Objective value at epoch t=12 is 0.4977123565144076\n",
      "Objective value at epoch t=13 is 0.4994299508142313\n",
      "Objective value at epoch t=14 is 0.49629504753107545\n",
      "Objective value at epoch t=15 is 0.49434307833780455\n",
      "Objective value at epoch t=16 is 0.4961412506090531\n",
      "Objective value at epoch t=17 is 0.4912495598633437\n",
      "Objective value at epoch t=18 is 0.49361863128786476\n",
      "Objective value at epoch t=19 is 0.4910857766368915\n",
      "Objective value at epoch t=20 is 0.4905626856876948\n",
      "Objective value at epoch t=21 is 0.48990717293683056\n",
      "Objective value at epoch t=22 is 0.48890718959510016\n",
      "Objective value at epoch t=23 is 0.4889091295181666\n",
      "Objective value at epoch t=24 is 0.4878130283748875\n",
      "Objective value at epoch t=25 is 0.48762264068773264\n",
      "Objective value at epoch t=26 is 0.4867569305867723\n",
      "Objective value at epoch t=27 is 0.48652146305396365\n",
      "Objective value at epoch t=28 is 0.4861376296969806\n",
      "Objective value at epoch t=29 is 0.48564905306823797\n",
      "Objective value at epoch t=30 is 0.4853892882607675\n",
      "Objective value at epoch t=31 is 0.4851277845902553\n",
      "Objective value at epoch t=32 is 0.48480939596771344\n",
      "Objective value at epoch t=33 is 0.4845457541507089\n",
      "Objective value at epoch t=34 is 0.4842988722434275\n",
      "Objective value at epoch t=35 is 0.4840647692967365\n",
      "Objective value at epoch t=36 is 0.48395271571010134\n",
      "Objective value at epoch t=37 is 0.48372963369810823\n",
      "Objective value at epoch t=38 is 0.4836014701543706\n",
      "Objective value at epoch t=39 is 0.48346378873981477\n",
      "Objective value at epoch t=40 is 0.48331412661620854\n",
      "Objective value at epoch t=41 is 0.48321980921052105\n",
      "Objective value at epoch t=42 is 0.48311609010493467\n",
      "Objective value at epoch t=43 is 0.4830207039947643\n",
      "Objective value at epoch t=44 is 0.4829347675440626\n",
      "Objective value at epoch t=45 is 0.48285679978996815\n",
      "Objective value at epoch t=46 is 0.4827867214121316\n",
      "Objective value at epoch t=47 is 0.48273319056723824\n",
      "Objective value at epoch t=48 is 0.48267625445739204\n",
      "Objective value at epoch t=49 is 0.4826230441142217\n",
      "Objective value at epoch t=50 is 0.48258128959132723\n",
      "Objective value at epoch t=51 is 0.4825387160134745\n",
      "Objective value at epoch t=52 is 0.4825028331593244\n",
      "Objective value at epoch t=53 is 0.48246870613505743\n",
      "Objective value at epoch t=54 is 0.4824390371910184\n",
      "Objective value at epoch t=55 is 0.48241192404600514\n",
      "Objective value at epoch t=56 is 0.48238869362257547\n",
      "Objective value at epoch t=57 is 0.48236679355988343\n",
      "Objective value at epoch t=58 is 0.4823472434893074\n",
      "Objective value at epoch t=59 is 0.4823298945068717\n",
      "Objective value at epoch t=60 is 0.4823138329739344\n",
      "Objective value at epoch t=61 is 0.4822996217728962\n",
      "Objective value at epoch t=62 is 0.4822865213641377\n",
      "Objective value at epoch t=63 is 0.48227487976515804\n",
      "Objective value at epoch t=64 is 0.48226431740797926\n",
      "Objective value at epoch t=65 is 0.48225497861285865\n",
      "Objective value at epoch t=66 is 0.4822466225509139\n",
      "Objective value at epoch t=67 is 0.48223896494297575\n",
      "Objective value at epoch t=68 is 0.48223211700683943\n",
      "Objective value at epoch t=69 is 0.48222593484146714\n",
      "Objective value at epoch t=70 is 0.4822203503173764\n",
      "Objective value at epoch t=71 is 0.4822153090379235\n",
      "Objective value at epoch t=72 is 0.48221080551799955\n",
      "Objective value at epoch t=73 is 0.4822066735756178\n",
      "Objective value at epoch t=74 is 0.48220310295584834\n",
      "Objective value at epoch t=75 is 0.48219978833604704\n",
      "Objective value at epoch t=76 is 0.4821968345721534\n",
      "Objective value at epoch t=77 is 0.4821941684973032\n",
      "Objective value at epoch t=78 is 0.48219177032327537\n",
      "Objective value at epoch t=79 is 0.4821896082288295\n",
      "Objective value at epoch t=80 is 0.4821876647161033\n",
      "Objective value at epoch t=81 is 0.4821859091351214\n",
      "Objective value at epoch t=82 is 0.4821843400853087\n",
      "Objective value at epoch t=83 is 0.4821829193643228\n",
      "Objective value at epoch t=84 is 0.48218164203035707\n",
      "Objective value at epoch t=85 is 0.48218049674271557\n",
      "Objective value at epoch t=86 is 0.48217946222452956\n",
      "Objective value at epoch t=87 is 0.4821785339061244\n",
      "Objective value at epoch t=88 is 0.4821776959076107\n",
      "Objective value at epoch t=89 is 0.4821769413460687\n",
      "Objective value at epoch t=90 is 0.48217626446749395\n",
      "Objective value at epoch t=91 is 0.4821756534233459\n",
      "Objective value at epoch t=92 is 0.4821751040571094\n",
      "Objective value at epoch t=93 is 0.48217460968565395\n",
      "Objective value at epoch t=94 is 0.48217416416478615\n",
      "Objective value at epoch t=95 is 0.4821737639937683\n",
      "Objective value at epoch t=96 is 0.4821734034549824\n",
      "Objective value at epoch t=97 is 0.4821730790766905\n",
      "Objective value at epoch t=98 is 0.482172786897795\n",
      "Objective value at epoch t=99 is 0.4821725242920583\n"
     ]
    }
   ],
   "source": [
    "lam = 1E-6\n",
    "stepsize = 0.1\n",
    "w, objvals_sgd1 = sgd(x_train, y_train, lam, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD with SGD\n",
    "\n",
    "Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "\n",
    "epochs_gd = range(len(objvals_gd1))\n",
    "epochs_sgd = range(len(objvals_sgd1))\n",
    "\n",
    "line0, = plt.plot(epochs_gd, objvals_gd1, '--b', LineWidth=4)\n",
    "line1, = plt.plot(epochs_sgd, objvals_sgd1, '-r', LineWidth=2)\n",
    "plt.xlabel('Epochs', FontSize=20)\n",
    "plt.ylabel('Objective Value', FontSize=20)\n",
    "plt.xticks(FontSize=16)\n",
    "plt.yticks(FontSize=16)\n",
    "plt.legend([line0, line1], ['GD', 'SGD'], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('compare_gd_sgd.pdf', format='pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: d-by-1 matrix\n",
    "#     X: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    xw = numpy.dot(X, w)\n",
    "    f = numpy.sign(xw)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classification error is 0.221875\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error\n",
    "f_train = predict(w, x_train)\n",
    "diff = numpy.abs(f_train - y_train) / 2\n",
    "error_train = numpy.mean(diff)\n",
    "print('Training classification error is ' + str(error_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification error is 0.203125\n"
     ]
    }
   ],
   "source": [
    "# evaluate test error\n",
    "f_test = predict(w, x_test)\n",
    "diff = numpy.abs(f_test - y_test) / 2\n",
    "error_test = numpy.mean(diff)\n",
    "print('Test classification error is ' + str(error_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker :\n",
    "    def __init__ (self, x, y):\n",
    "        self.x = x # n-by-d \n",
    "        self.y = y # n by 1\n",
    "        n = x.shape[0] # no. of samples\n",
    "        d = x.shape[1] # no. of features\n",
    "        w = numpy.zeros((d,1))\n",
    "        \n",
    "    def set_param(w):\n",
    "        w = w\n",
    "        \n",
    "    def loss():\n",
    "        yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "        yxw = numpy.dot(yx, w) # n-by-1 matrix\n",
    "        vec1 = numpy.exp(-yxw) # n-by-1 matrix\n",
    "        vec2 = numpy.log(1 + vec1) # n-by-1 matrix\n",
    "        return numpy.sum(vec2)\n",
    "    \n",
    "    def gradient():\n",
    "        yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "        yxw = numpy.dot(yx, self.w) # n-by-1 matrix\n",
    "        vec1 = numpy.exp(yxw) # n-by-1 matrix\n",
    "        vec2 = numpy.divide(yx, 1+vec1) # n-by-d matrix\n",
    "        g = -numpy.sum(vec2, axis=0).reshape(self.d, 1) # d-by-1 matrix\n",
    "       #g = vec3 + lam * w\n",
    "        return g \n",
    "    \n",
    "    def st_gradient():\n",
    "        yx = numpy.multiply(y, x) # n-by-d matrix\n",
    "        yxw = numpy.dot(yx, self.w) # n-by-1 matrix\n",
    "        vec1 = numpy.exp(yxw)\n",
    "        sg = -yx.T / (1 + vec1)\n",
    "        return sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server:\n",
    "    def __init__(self, m, n, d):\n",
    "        self.m = m # no. of worker nodes\n",
    "        self.n = n # no. of training samples\n",
    "        self.d = d # no. of feautres\n",
    "        w = numpy.zeros((d,1))\n",
    "        g = numpy.zeros((d,1))\n",
    "        v = numpy.zeros((d,1))\n",
    "        loss = 0 # loss function value\n",
    "        obj = 0 # objective function value\n",
    "        \n",
    "    def broadcast(w):\n",
    "        return w\n",
    "    \n",
    "    # sum loss func and gradients evaluated by workers\n",
    "    #args\n",
    "    # grads : a list of d-by-1 vectors\n",
    "    # loss  : a list of scalars\n",
    "    \n",
    "    def aggregate(grads, losses):\n",
    "        g = numpy.zeros((d, 1))\n",
    "        loss = 0\n",
    "        for k in range(m):\n",
    "            g += grads[k]\n",
    "            loss += losses[k]\n",
    "            \n",
    "    # compute the gradient (from the loss and regularization)\n",
    "    def gradient(lam):\n",
    "        g = g / n + lam * w\n",
    "        \n",
    "    # compute the objective function(sum of loss and regularization)\n",
    "    def objective (lam):\n",
    "        reg = lam / 2 * numpy.sum(w * w)\n",
    "        obj = loss / n + reg\n",
    "        return obj\n",
    "    \n",
    "    def st_gradient(lam):\n",
    "        sg = sg  + lam * w\n",
    "    \n",
    "    # update the model parameters using Accelerated GD\n",
    "    # args\n",
    "    # alpha : learning rate( step size)\n",
    "    # beta : momentum parameter\n",
    "    def agd (alpha, beta):\n",
    "        v *= beta\n",
    "        v += g\n",
    "        sw -= alpha * v\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# create a server and m worker nodes:\n",
    "\n",
    "def create_server_workers (m, x, y):\n",
    "    n, d = x.shape\n",
    "    n = math.floor(n / m)\n",
    "    print(m,n,d)\n",
    "    server = Server(m, n, d)\n",
    "    workers = []\n",
    "    \n",
    "    for i in range(m):\n",
    "        indices = list(range(i * n, (i+1)*n))\n",
    "        worker = Worker(x[indices, :], y[indices,:])\n",
    "        workers.append(worker)\n",
    "        \n",
    "    return server, workers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 160 9\n"
     ]
    }
   ],
   "source": [
    "m = 4\n",
    "server, workers = create_server_workers(m, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lam = 1E-6 #regularization parameter\n",
    "alpha = 1E-1 #learning rate\n",
    "beta = 0.9\n",
    "max_epoch = 100\n",
    "\n",
    "for t in range(max_epoch):\n",
    "    # step 1: broadcast\n",
    "    w = server.broadcast()\n",
    "#     w = numpy.zeros((d,1))\n",
    "    print(w)\n",
    "    for i in range(m):\n",
    "#         workers[i].set_param(server.broadcast())\n",
    "        workers[i].set_param(w)\n",
    "        \n",
    "        \n",
    "    # step 2: workers local computations\n",
    "    grads =[]\n",
    "    losses =[]\n",
    "    for i in range(m):\n",
    "        g = workers[i].gradient()\n",
    "        grads.append(g)\n",
    "        l = workers[i].loss()\n",
    "        losses.append(l)\n",
    "        \n",
    "    # step 3: aggregate the workers outputs\n",
    "    server.aggregate(grads, losses)\n",
    "    \n",
    "    # step 4: server update the model paramters\n",
    "    server.gradient(lam)\n",
    "    obj = server.objective(lam)\n",
    "    print('Objective function value for t ='+str(t) + ' is ' + str(obj))\n",
    "    server.agd(alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plot and compare GD, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are required to compare the following algorithms:\n",
    "\n",
    "- Gradient descent (GD)\n",
    "\n",
    "- SGD\n",
    "\n",
    " - GD with q=1, GD with q=8, SGD with q=1, SGD with q=8\n",
    "\n",
    "Follow the code in Section 4 to plot ```objective function value``` against ```epochs```. There should be four curves in the plot; each curve corresponds to one algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Logistic regression with $\\ell_2$-norm regularization is a strongly convex optimization problem. All the algorithms will converge to the same solution. **In the end, the ``objective function value`` of the 4 algorithms will be the same. If not the same, your implementation must be wrong. Do NOT submit wrong code and wrong result!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 4 curves:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "epochs_gd = range(len(objvals_gd1))\n",
    "epochs_sgd = range(len(objvals_sgd1))\n",
    "epochs_gd = range(len(objvals_gd8))\n",
    "epochs_sgd = range(len(objvals_sgd8))\n",
    "\n",
    "line0, = plt.plot(epochs_gd, objvals_gd1, '--b', LineWidth=3)\n",
    "line1, = plt.plot(epochs_sgd, objvals_sgd1, '-r', LineWidth=4)\n",
    "line2, = plt.plot(epochs_mbsgd8, objvals_gd8, 'g', LineWidth=2)\n",
    "line3, = plt.plot(epochs_mbsgd64, objvals_sgd8, '--y', LineWidth=3)\n",
    "plt.xlabel('Epochs', FontSize=20)\n",
    "plt.ylabel('Objective Value', FontSize=20)\n",
    "plt.xticks(FontSize=16)\n",
    "plt.yticks(FontSize=16)\n",
    "plt.legend([line0, line1, line2, line3], ['GD1', 'SGD1', 'GD8', 'SGD8'], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('compare_gd_sgd_mbsgd8_mbsgd64.pdf', format='pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
